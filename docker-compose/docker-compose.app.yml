services:
  # HDFS Namenode
  namenode:
    image: alexflames77/custom-hadoop-namenode:3.3.6-java17
    container_name: namenode
    hostname: namenode
    environment:
      - CLUSTER_NAME=hadoopvideo
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - CORE_CONF_hadoop_http_staticuser_user=root
      - CORE_CONF_hadoop_proxyuser_hue_hosts=*
      - CORE_CONF_hadoop_proxyuser_hue_groups=*
      - HDFS_CONF_dfs_replication=1
      - HDFS_CONF_dfs_namenode_name_dir=/hadoop/dfs/name
      - HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
    # env_file:
    #   - ./hadoop.env
    command: ["hdfs", "namenode"]
    user: "hadoop"
    volumes:
      - namenode-data:/hadoop/dfs/name
      - hadoop-logs:/usr/local/hadoop/logs
      # - ./hdfs-site.xml:/etc/hadoop/conf/hdfs-site.xml:ro
      # - ./core-site.xml:/usr/local/hadoop/etc/hadoop/core-site.xml:ro
    ports:
      - "50070:50070"  # NameNode web UI
      - "8020:8020"    # HDFS port
    networks:
      - kafka-net
    restart: always
    # deploy:
    #   resources:
    #     limits:
    #       cpus: "1"
    #       memory: "2g"

  # HDFS Datanode
  datanode:
    image: alexflames77/custom-hadoop-datanode:3.3.6-java17
    container_name: datanode
    hostname: datanode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - HDFS_CONF_dfs_datanode_data_dir=/hadoop/dfs/data
      - HDFS_CONF_dfs_replication=1
      - HDFS_CONF_dfs_permissions_enabled=false
    # env_file:
    #   - ./hadoop.env
    command: ["hdfs", "datanode"]
    user: "hadoop"
    volumes:
      - datanode-data:/hadoop/dfs/data
      - hadoop-logs:/usr/local/hadoop/logs
      # - ./hdfs-site.xml:/etc/hadoop/conf/hdfs-site.xml:ro
      # - ./core-site.xml:/usr/local/hadoop/etc/hadoop/core-site.xml:ro
    ports:
      - "50075:50075"  # DataNode web UI
      - "1004:1004"    # DataNode auxiliary port
      - "1006:1006"    # DataNode auxiliary port
    depends_on:
      - namenode
    networks:
      - kafka-net
    restart: always
    # deploy:
    #   resources:
    #     limits:
    #       cpus: "1"
    #       memory: "2g"

  # Kafka Brokers
  kafka-1:
    image: bitnami/kafka:3.8.0
    container_name: kafka-1
    ports:
      - "9092:9092"
    environment:
      - KAFKA_ENABLE_KRAFT=yes
      - KAFKA_KRAFT_CLUSTER_ID=d8ce1515-401e-44d4-a444-1b6dba479047
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_HEAP_OPTS=-Xmx2G -Xms2G
    volumes:
      - ./server-1.properties:/opt/bitnami/kafka/config/server.properties:ro
    networks:
      - kafka-net
    restart: always
    # deploy:
    #   resources:
    #     limits:
    #       cpus: "1"
    #       memory: "2g"
  kafka-2:
    image: bitnami/kafka:3.8.0
    container_name: kafka-2
    ports:
      - "9095:9095"
    environment:
      - KAFKA_ENABLE_KRAFT=yes
      - KAFKA_KRAFT_CLUSTER_ID=d8ce1515-401e-44d4-a444-1b6dba479047
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_HEAP_OPTS=-Xmx2G -Xms2G
    volumes:
      - ./server-2.properties:/opt/bitnami/kafka/config/server.properties:ro
    networks:
      - kafka-net
    restart: always
    # deploy:
    #   resources:
    #     limits:
    #       cpus: "1"
    #       memory: "2g"

  # Kafka Service
  kafka-service:
    image: alexflames77/kafka_service:latest
    container_name: kafka-service
    environment:
      - BOOTSTRAP_SERVERS=kafka-1:9092,kafka-2:9095
    networks:
      - kafka-net
    ports:
      - "9091:9091"
    restart: always
    # deploy:
    #   resources:
    #     limits:
    #       cpus: "1"
    #       memory: "2g"
    depends_on:
      - kafka-1
      - kafka-2

  # Flink JobManager
  jobmanager:
    image: flink:1.17.1
    container_name: jobmanager
    hostname: jobmanager
    ports:
      - "8081:8081"
      - "6123:6123"
      - "6124:6124"
    command: jobmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        taskmanager.registration.timeout: 5 min
    networks:
      - kafka-net
    restart: always

  # Flink TaskManager
  taskmanager:
    image: flink:1.17.1
    container_name: taskmanager
    hostname: taskmanager
    command: taskmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        taskmanager.registration.timeout: 5 min
    networks:
      - kafka-net
    restart: always
    depends_on:
      - jobmanager

  # Flink Job
  flink-job:
    image: alexflames77/flink_job:latest
    container_name: flink-job
    environment:
      - FLINK_JOBMANAGER_HOST=jobmanager
      - FLINK_JOBMANAGER_PORT=8081
    command: >
      /bin/bash -c "
      java 
      --add-opens=java.base/java.util=ALL-UNNAMED 
      --add-opens=java.base/java.lang=ALL-UNNAMED 
      --add-opens=java.base/java.lang.invoke=ALL-UNNAMED 
      --add-opens=java.base/java.nio=ALL-UNNAMED 
      --add-opens=java.base/sun.nio.ch=ALL-UNNAMED 
      "
    networks:
      - kafka-net
    depends_on:
      - jobmanager
      - taskmanager
    restart: on-failure
    # deploy:
    #   resources:
    #     limits:
    #       cpus: "1"
    #       memory: "2g"

  # spark-job:
  #   image: alexflames77/spark_job:latest
  #   environment:
  #     - SPARK_MASTER_URL=spark://spark-master:7077
  #     - SPARK_APPLICATION_JAR_LOCATION=/opt/spark-apps/spark-ml-job.jar
  #     - SPARK_APPLICATION_MAIN_CLASS=SparkMLJob
  #   networks:
  #     - kafka-net
  #   restart: always
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "1"
  #         memory: "2g"
  #   depends_on:
  #     - kafka-1
  #     - kafka-2

  # Spark Master
  spark-master:
    image: bitnami/spark:3.5.4
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "7077:7077"
      - "8080:8080"
    networks:
      - kafka-net
    restart: always
    # deploy:
    #   resources:
    #     limits:
    #       cpus: "1"
    #       memory: "2g"

  # Spark Worker
  spark-worker:
    image: bitnami/spark:3.5.4
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
    depends_on:
      - spark-master
    networks:
      - kafka-net
    restart: always
    # deploy:
    #   resources:
    #     limits:
    #       cpus: "1"
    #       memory: "2g"

  # Spark Job
  spark-job:
    image: alexflames77/spark_job:latest
    container_name: spark-job
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
    volumes:
      - ./models/saved_model:/opt/spark-apps/models/saved_model
    depends_on:
      - spark-master
    networks:
      - kafka-net
    restart: always
    # deploy:
    #   resources:
    #     limits:
    #       cpus: "1"
    #       memory: "2g"

  # Prometheus
  prometheus:
    image: prom/prometheus
    container_name: prometheus
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      -  prometheus-data:/prometheus
    ports:
      - "9090:9090"
    networks:
      - kafka-net
    restart: always
    # deploy:
    #   resources:
    #     limits:
    #       cpus: "1"
    #       memory: "2g"

  # Grafana
  grafana:
    image: grafana/grafana
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-storage:/var/lib/grafana
    networks:
      - kafka-net
    restart: always
    # deploy:
    #   resources:
    #     limits:
    #       cpus: "1"
    #       memory: "2g"

networks:
  kafka-net:
    driver: bridge

volumes:
  namenode-data:
    name: namenode-data
  datanode-data:
    name: datanode-data
  hadoop-logs:
    name: hadoop-logs
  grafana-storage:
    name: grafana-storage
  prometheus-data:
    name: prometheus-data